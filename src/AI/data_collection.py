###############################################################
# This script must be run the day after all games were played #
###############################################################

from datetime import datetime, timedelta
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
import requests

# This is used to update the list of players gathered from get_recap() with the correct spelling of player name (an ever growing list)
CORRECT_PLAYER_NAMES = {
    'John Peterka': 'JJ Peterka',
    'Emil Bemström': 'Emil Bemstrom',
    'Teuvo Teräväinen': 'Teuvo Teravainen'
}

def extract_players(url):
    headers = { 'User-Agent': UserAgent().random }
    req = requests.get(url=url, headers=headers)
    soup = BeautifulSoup(req.text, 'html.parser')

    stat_list = soup.find('tbody').find_all('tr')

    pp_point_players = []
    for i in stat_list:
        columns = i.find_all('td')
        player_name, player_team = i.find_all('a')
        
        # If the player is a goalie, skip
        if 'G' in columns[2]:
            continue 

        PPP = columns[24].text

        if int(PPP) > 0:
            pp_point_players.append({'name': player_name.text, 'team': player_team.text})

    return pp_point_players
        
# Returns a list of all players that scored a powerplay ploint the night before
def get_recap():
    # Returns the day before today in a string formated '2023-11-03'
    last_night = datetime.strftime(datetime.now() - timedelta(1), '%Y-%m-%d')

    url = f'https://www.quanthockey.com/nhl/seasons/nhl-players-stats.html?sd={last_night}&ed={last_night}'

    headers = { 'User-Agent': UserAgent().random }
    req = requests.get(url=url, headers=headers)
    soup = BeautifulSoup(req.text, 'html.parser')

    print(url)

    pages = soup.find('ul', 'pagination').find_all('a')
    pages.pop(0) # Remove the first pagination arrow for going back
    pages.pop(0) # Remove current page as it doesn't have proper formated href's
    pages.pop(len(pages) -1) # Remove the second pagination arrow for going forward

    # Go through each page of players to check for players that scored a powerplay point
    extracted_players = []
    for i in pages:
        href = i['href']
        params = str(href).split('(')[1].split(')')[0]
        p = params.replace("'", "").split(',')

        # Extract the players from the first page with hardcoded value of 1 where p[8] should be (page number) 
        # I HATE THIS; SUPER UGLY
        if int(p[8]) == 2 :
            url = f'https://www.quanthockey.com/scripts/AjaxPaginate.php?cat={p[0]}&pos={p[1]}&SS={p[2]}&af={p[3]}&nat={p[4]}&st={p[5]}&sort={p[6]}&so={p[7]}&page={1}&league=NHL&lang=en&rnd=0&dt=2&sd={p[12]}&ed={p[13]}'
            extracted_players.append(extract_players(url=url))

        url = f'https://www.quanthockey.com/scripts/AjaxPaginate.php?cat={p[0]}&pos={p[1]}&SS={p[2]}&af={p[3]}&nat={p[4]}&st={p[5]}&sort={p[6]}&so={p[7]}&page={p[8]}&league=NHL&lang=en&rnd=0&dt=2&sd={p[12]}&ed={p[13]}'
        extracted_players.append(extract_players(url=url))
        print(f'Progress: ({int(p[8])}/{len(pages)}): {round((int(p[8])/len(pages) * 100))}%', end='\r') # Ain't pretty, but it works

    return np.concatenate(extracted_players)

def update_csv(extracted_players):
    cur_df = pd.read_csv('.\src\AI\\data\current.csv')
    df = pd.read_csv('.\lib\\ai_bum_list.csv')
    ex_p = pd.DataFrame([x for x in extracted_players], columns=['name', 'team'])

    ex_p['name'] = ex_p['name'].map(CORRECT_PLAYER_NAMES).fillna(ex_p['name'])
    players_to_update = df[df['skaterFullName'].isin(ex_p['name'])]
    not_in = ex_p[~ex_p['name'].isin(df['skaterFullName'])]

    print('Players not found:')
    print(not_in)

    # lambda staement generated by ChatGPT; updates the column 'scored' to 1 if the player scored a powerplay point
    df['scored'] = df.apply(lambda row: 1 if row['skaterFullName'] in players_to_update['skaterFullName'].values and 
                                             row['teamAbbrevs'] in players_to_update['teamAbbrevs'].values else row['scored'], axis=1)
    
    final_df = pd.concat([cur_df, df], ignore_index=True)
    final_df.to_csv('.\src\AI\\data\current.csv', index=False)

if __name__ == "__main__":
    player_list = get_recap()
    update_csv(extracted_players=player_list)
    print('Successfully updated player data...')
    exit(0)